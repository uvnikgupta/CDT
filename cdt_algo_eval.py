import multiprocessing
import uuid, math, time, datetime, pickle, os, shutil, yaml, glob, sys
import numpy as np
import pandas as pd
import networkx as nx
from networkx.drawing.nx_agraph import graphviz_layout
import matplotlib.pyplot as plt
import openpyxl
from openpyxl.drawing.image import Image
from io import BytesIO
from PIL import Image as PILImage
from sklearn.preprocessing import StandardScaler
from cdt.causality.graph import (GS, GIES, PC, SAM, IAMB, Inter_IAMB, 
                                 Fast_IAMB, MMPC, LiNGAM, CAM, CCDr)
from cdt.metrics import precision_recall, SID, SHD
from scm.dynamic_scm import DynamicSCM
from scmodels import SCM

plt.switch_backend('agg')

# Global variables
plots_file = "logs/plots_cdt_algo.xlsx"
plots_sheet_name = "plots"
config_sheet_name = "configs"
log_file ='logs/cdt_algo_eval.log'
data_folder = "data_cdt_algo_eval"
config_file = "dag_generation_configs.yml"
cdt_algos_file = "cdt_algos.yml"
metrics = {
    "aupr":'round(precision_recall(orig_dag, algo_dag)[0], 2)', 
    "sid":'int(SID(orig_dag, algo_dag))',
    "shd":'int(SHD(orig_dag, algo_dag))',
    "rt":'round(time.time() - start, 2)'}
sample_sizes = [1000, 10000, 20000, 50000, 100000]
num_iterations = 3
row = 1
   
def log_progress(message):
    with open(log_file, "a+") as f:
        ts = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        f.writelines(f"{ts} - {message}\n")

def consolidate_scores(scores):
    for algo in scores.keys():
        for metric in metrics.keys():
            mean = round(np.array(scores[algo][metric]).mean(), 2)
            std = round(np.array(scores[algo][metric]).std(), 2)
            scores[algo][metric] = (mean, std)

def update_scores(scores, results):
    for result in results:
        algo_name = result[0]
        with open(result[1], "rb") as file:
            dag = pickle.load(file)
        scores[algo_name]['dag'] = dag
        for metric in metrics:
            scores[algo_name][metric].append(result[2][metric])

def train_non_parallable_algos(algos, data_file, conf, orig_scm_dists, results):
    # Then train the algos that cannot be run in parallel. These are 
    # the NN based algos that spawn their own process hence cannot be 
    # run from within an already spawned process
    results = []
    for algo_meta_data in algos:
        if not algo_meta_data["parallel"]:
            result = train_algo(algo_meta_data, data_file, conf["name"], 
                                orig_scm_dists)
            results.append(result)
    return results

def get_algo_metrics(algo_name, orig_dag, algo_dag, start):
    retval ={}
    if algo_dag is not None:
        for metric, formula in metrics.items():
            if "sam" in algo_name.lower() and "sid" in metric.lower():
                # Calculating SID for DAGs generated by SAM does not finish
                # in reasonable time. Hence setting this metric to inf for SAM
                retval[metric] = np.inf
            else:
                retval[metric] = eval(formula)
    else:
        for metric, formula in metrics.items():
            retval[metric] = 0
    return retval

def scale_data(data):
    columns = data.columns
    scaler = StandardScaler()
    data = scaler.fit_transform(data)
    data = pd.DataFrame(data=data, columns=columns)
    return data

def train_algo(algo_meta_data, data_file, conf_name, orig_scm_dists):
    uid = uuid.uuid4()
    scm = SCM(orig_scm_dists)
    algo_name = algo_meta_data["name"]
    
    with open(data_file, "rb") as file:
        data = pickle.load(file)

    if "scale_data" in algo_meta_data and algo_meta_data["scale_data"]:
        data = scale_data(data)

    context = f"{algo_name} for {conf_name} using {len(data)} samples - {uid}"
    log_progress(f"Start Training : {context}")
    
    start = time.time()
    algo = eval(algo_meta_data["model"])
    try:
        algo_dag = algo.predict(data)
        metrics = get_algo_metrics(algo_name, scm.dag, algo_dag, start)
        log_progress(f"End Training : {context}")
    except Exception as e:
        algo_dag = f"EXCEPTION in training {context}"
        metrics = get_algo_metrics(algo_name, None, None, 0)
        log_progress(f"EXCEPTION in training {context}\n{e}")

    dag_file_name = f"temp/{algo_meta_data['name']}.dag"
    with open(dag_file_name, "wb") as file:
        pickle.dump(algo_dag, file)

    return ((algo_meta_data['name'], dag_file_name, metrics))
    
def get_training_args_list(algos, data_file, conf, orig_scm_dists):
    # Create the argument list for training algos in parallel
    args_list = []
    for algo_meta_data in algos:
        if algo_meta_data["parallel"] == True:
            args_list.append((algo_meta_data, data_file, conf["name"], orig_scm_dists))
    return args_list

def get_timeout_value(conf, num_samples):
    if isinstance(conf["nodes"], list):
        num_nodes = sum(conf["nodes"])
    else:
        num_nodes = conf["nodes"]
    
    timeout = int(math.exp(int(math.log(num_nodes * 2)) + 
                           int(math.log(math.sqrt((num_samples))))))
    return timeout

def init_scores(models):
    scores = {}
    for algo_meta_data in models:
        algo_name = algo_meta_data["name"]
        scores[algo_name] = {}
        scores[algo_name]["dag"] = None
        for metric in metrics:
            scores[algo_name][metric] = []
    return scores

def train_algos_for_sample_size(algos, data_folder, num_samples, 
                                conf, orig_scm_dists):
    scores = init_scores(algos)
    num_processes = max(1, min(len(algos), multiprocessing.cpu_count() - 1))
    timeout = get_timeout_value(conf, num_samples)

    for iter in range(num_iterations):
        log_progress(f"\n**** Starting training iteration {iter} for \
{num_samples} samples ****")
        data_file = f"{data_folder}/{conf['name']}_{num_samples}_{iter}.data"
        args_list = get_training_args_list(algos, data_file, conf, orig_scm_dists)
        try:
            # First Train algos in parallel with timeout
            pool = multiprocessing.Pool(processes=num_processes)
            results = [pool.apply_async(train_algo, args=args) 
                        for args in args_list]
            results = [result.get(timeout=timeout) for result in results]
            result = train_non_parallable_algos(algos, data_file, conf, 
                                                      orig_scm_dists, results)
            if result: results.append(result)
            update_scores(scores, results)
        except multiprocessing.TimeoutError:
            log_progress(f"**TIMEOUT after {timeout} secs in \
iteration {iter} for {conf['name']} using {num_samples}")
            
        log_progress(f"\n==== Completed training iteration {iter} for \
{num_samples} samples ====")
    return scores

def get_algos_for_training(conf):
    with open(cdt_algos_file) as file:
        models = yaml.safe_load(file)

    algos = []
    for algo_meta_data in models:
        if "type" in algo_meta_data:
            if "type" in conf and algo_meta_data["type"] == conf["type"]:
                # add type specifc algos
                algos.append(algo_meta_data)
            elif "type" not in conf and algo_meta_data["type"] == 1:
                # if conf is of mixed type add continuous alogs
                algos.append(algo_meta_data)
        else: # add common algos
            algos.append(algo_meta_data)
    return algos

def populate_cdt_algos_scores_for_config(scm_dists, conf, data_folder):
    global row
    algos = get_algos_for_training(conf)

    # Run algo training for various sample sizes for the config
    for num_samples in sample_sizes:
        # Train CDT algos and get their DAGs and run durations
        scores = train_algos_for_sample_size(algos, data_folder, num_samples, 
                                             conf, scm_dists)
        consolidate_scores(scores)
        save_plots_and_scores(plots_file, 1, scores, conf, num_samples)  
        row = row + 1
        
def save_config_to_xl(configs_sheet, conf):
    row = 1
    while configs_sheet.cell(row=row, column=1).value is not None:
        row +=1
    configs_sheet.cell(row=row, column=1, value=str(conf))    

def get_score_text(score_data):
    text = ""
    for metric, values in score_data.items():
        if metric != "dag":
            text += f"{metric}:{values}\n"
    return text

def get_node_colors(dag):
    nodes = dag.nodes()
    colors = ['#F9E79F','#A3E4D7','#D7BDE2','#A9CCE3','#DAF7A6',
              '#EBDEF0','#EDBB99','#3498DB','#EC7063','#E5E8E8',]
    node_colors = []
    prev_level = ""
    index = -1
    for node in nodes:
        cur_level = node[0]
        if cur_level != prev_level:
            prev_level = cur_level
            index += 1
        if index > len(colors) - 1:
            index = 0
        node_colors.append(colors[index])
    return node_colors

def save_plots_and_scores_to_xl(plots_sheet, col, scores, conf, num_samples=""):
    for algo, score_data in scores.items():
        # Since all the DAGs have the same number of nodes with the 
        # same names, node colors will of any one DAG will be applicable
        # for all DAGs. Since the only way to access an item in a python 
        # dict is via the key, hence this sing loop for loop
        node_colors = get_node_colors(score_data["dag"])
        break

    for algo, score_data in scores.items():
        dag = score_data["dag"]
        fig, ax = plt.subplots(1, 1, figsize=(3,2), dpi=150)
        ax.set_title(f"{conf['name']}_{algo}_{num_samples}")

        if isinstance(dag, nx.classes.digraph.DiGraph):
            pos = graphviz_layout(dag, prog="dot")
            nx.draw(dag, pos=pos, ax=ax, with_labels=True, node_size=150, 
                    node_color=node_colors, font_size=7, alpha=0.5)
            text = get_score_text(score_data)
            fig.text(0.65, 0.05, text, fontsize=10, color='black')
        else:
            text = dag
            fig.text(0.2, 0.2, text, fontsize=8, color='red')
        
        fig.savefig("plot.png")
        plt.close(fig)
        
        pil_image = PILImage.open('plot.png')
        image_bytes = BytesIO()
        pil_image.save(image_bytes, format='PNG')
        image_bytes.seek(0)
        image = Image(image_bytes)

        target_cell = chr(65 + col) + str(row)
        width, height = 35, 110
        plots_sheet.column_dimensions[target_cell[0]].width = width
        plots_sheet.row_dimensions[int(target_cell[1:])].height = height
        image.width = width * 6.8
        image.height = height * 1.3

        plots_sheet.add_image(image, target_cell)
        pil_image.close()
        os.remove('plot.png')
        col = col + 1

def check_num_rows_in_xl_and_move_it(plots_file):
    global row
    if row > 10:
        prefix = datetime.datetime.now().strftime("%d%H%M%S")
        new_file_path = f"{os.path.splitext(plots_file)[0]}_{prefix}.xlsx"
        shutil.move(plots_file, new_file_path)
        row = 1
        log_progress(f"The file '{plots_file}' has been renamed to '{new_file_path}' \
due to exceeding the maximum size.")

def get_xl_workbook_for_plots(plots_file):
    workbook = None
    if os.path.exists(plots_file):
        check_num_rows_in_xl_and_move_it(plots_file)

    try:
        workbook = openpyxl.load_workbook(plots_file)
    except:
        workbook = openpyxl.Workbook()
        plots_sheet = workbook.active
        plots_sheet.title = plots_sheet_name
        configs_sheet = workbook.create_sheet()
        configs_sheet.title = config_sheet_name
    return workbook

def save_plots_and_scores(plots_file, col, scores, conf, num_samples=""):
    log_progress("Saving DAGs - Start")
    workbook = get_xl_workbook_for_plots(plots_file)
    
    plots_sheet = workbook[plots_sheet_name]
    save_plots_and_scores_to_xl(plots_sheet, col, scores, conf, num_samples)

    if col == 0:
        configs_sheet = workbook[config_sheet_name]
        save_config_to_xl(configs_sheet, conf)

    workbook.save(plots_file)
    workbook.close()
    log_progress("Saving DAGs - Done")

def generate_data_for_config(num_samples, iter, dists_file_path):
    folder = dists_file_path.split("/")[0]
    conf_name = dists_file_path.split("/")[1].split(".")[0]
    data_file = f"{folder}/{conf_name}_{num_samples}_{iter}.data"
    
    if os.path.exists(data_file):
        log_progress(f"{data_file} already exists.")
        return data_file
    
    with open(dists_file_path, "rb") as file:
        scm_dists = pickle.load(file)
    scm = SCM(scm_dists)
    trials = 10
    while trials:
        msg = f"{num_samples} samples for {conf_name}"
        try:
            log_msg = f"Iter {iter}: Starting data generation of {msg}"
            log_progress(log_msg)
            data = scm.sample(num_samples)

            with open(data_file, "wb") as file:
                pickle.dump(data, file)

            log_msg = f"Iter {iter}: Completed data generation of {msg}"
            log_progress(log_msg)
            break
        except Exception as e:
            log_msg = f"Iter {iter}: EXCEPTION in data generation. Trying again {msg}"
            log_progress(log_msg)
            trials -= 1
            continue
    return data_file

def get_args_for_data_generation_process(dists_file_paths, num_iterations, 
                                         sample_sizes):
    args = [(num_samples, iter, path) for path in dists_file_paths 
                                            for iter in range(num_iterations) 
                                                for num_samples in sample_sizes]
    return args

def get_scm(config):
    input_nodes = config["nodes"]
    dSCM = eval(config["dSCM"])
    scm = dSCM.create(input_nodes)
    return scm, dSCM.get_scm_dists()

def check_and_handle_dists_creation(dists_file, conf, data_folder):
    if not conf["force_data_generation"]:
        if os.path.exists(dists_file):
            log_progress(f"{dists_file} already exists. \
To generate new distribution set 'force_data_generation' to true")
            return False

    pattern = f"{conf['name']}_*.data"
    data_files = glob.glob(os.path.join(data_folder, pattern))
    for file in data_files:
        os.remove(file)
    return True

def get_algo_eval_configs(config_file):
    with open(config_file,"r") as file:
        configs = yaml.safe_load(file)
    return configs
    
def generate_and_save_scm_dists(config_file, data_folder):
    dists_file_paths = []
    for conf in get_algo_eval_configs(config_file):
        dists_file = f"{data_folder}/{conf['name']}.dists"
        dists_file_paths.append(dists_file)

        create = check_and_handle_dists_creation(dists_file, conf, data_folder)
        if create:
            _, scm_dists = get_scm(conf)
            with open(dists_file, "wb") as file:
                pickle.dump(scm_dists, file)   
    return dists_file_paths

def generate_all_data(data_folder, config_file):
    folder = data_folder
    if not os.path.exists(folder):
        os.makedirs(folder)

    dists_file_paths = generate_and_save_scm_dists(config_file, data_folder)
    args = get_args_for_data_generation_process(dists_file_paths, 
                                                num_iterations,
                                                sample_sizes)
    num_processes = max(1, min(len(args), multiprocessing.cpu_count() - 1))
    pool = multiprocessing.Pool(processes=num_processes)
    results = pool.starmap(generate_data_for_config, args)
    results = [result for result in results]
    return dists_file_paths

if __name__ == "__main__":
    arguments = sys.argv

    temp_folder = "temp"
    if not os.path.exists(temp_folder):
        os.makedirs(temp_folder)

    print(f"argument : {arguments[1]}")
    if len(arguments) > 1 and int(arguments[1]) == 1:
        for filename in os.listdir("logs"):
            file_path = os.path.join("logs", filename)
            if os.path.isfile(file_path):
                os.remove(file_path)
        dists_file_paths = generate_all_data(data_folder, config_file)

    for conf in get_algo_eval_configs(config_file):
        dists_file = f"{data_folder}/{conf['name']}.dists"
        with open(dists_file, "rb") as file:
            scm_dists = pickle.load(file)
        scm = SCM(scm_dists)
        save_plots_and_scores(plots_file, 0, 
                           {"Original":{"dag": scm.dag}}, 
                           conf)
        
        populate_cdt_algos_scores_for_config(scm_dists, conf, data_folder)